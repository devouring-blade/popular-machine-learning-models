# Mathematical foundation
## 1. Objective

Find a new axis ð‘£ (unit vector) so that when projecting data ð‘¥ð‘– onto it, the variance of the projected data is maximized.

Reduce dimensionality by keeping axes with the largest variance.

## 2. Data Standardization

Gaussian standardization: ð‘¥ð‘– = (ð‘¥ð‘–âˆ’mean(ð‘¥ð‘–)) / std(ð‘¥ð‘–).

After this step, mean(ð‘¥ð‘–) = 0.

Meaning: centering data around the origin helps identify the â€œdirection of maximum spreadâ€ and balances features with different ranges.

## 3. Projecting Data onto the New Axis

For one point: ð‘§ð‘– = ð‘£^ð‘‡.ð‘¥ð‘–

For all data: ð‘ = ð‘‹.ð‘£

## 4. Variance of Projected Data

Var(z) = (1/n).Î£(zi âˆ’ zÌ„)Â²

After centering (Î¼=0): Var(z) = (1/n).Î£(ziÂ²) = (1/n).(Xv)^T.(Xv) = v^T.A.v

where A = (1/n).X^T.X (covariance matrix)

## 5. Constrained Optimization

Maximize J(v) = v^T.A.v

Constraint: v^T.v = 1 (unit vector has |v| = 1)

Lagrangian: L(v, Î») = v^T.A.v âˆ’ Î».(v^T.v âˆ’ 1)

Derivative to find the extremum to maximize with respect to ð‘£: A.v = Î».v

## 6. Finding Eigenvalues & Eigenvectors

Equation: (A âˆ’ Î»I).v = 0

Non-trivial solutions exist only if det(A âˆ’ Î»I) = 0

Solve for Î» (eigenvalues) and v (eigenvectors), then normalize v

## 7. Interpretation of Results

Eigenvector v = direction of the new axis

Eigenvalue Î» = variance along that axis (because Var(z) = v^T.A.v and A.v = Î».v => Var(z) = v^T.Î».v = Î».(v^T.v), but v^T.v = 1 => Var(z) = Î»

## 8. Principal Components

Sort eigenvalues in descending order

Choose k eigenvectors with largest Î» â†’ Vk

Project data: Z = X.Vk

#
